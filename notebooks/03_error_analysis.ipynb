{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "## AI Document Intelligence - Week 1\n",
    "\n",
    "This notebook analyzes OCR errors by comparing with ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.ocr import OCRErrorAnalyzer\n",
    "from src.utils.file_utils import read_json, list_files\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Error Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path.cwd().parent / \"dataset\"\n",
    "\n",
    "analyzer = OCRErrorAnalyzer(\n",
    "    labels_dir=str(dataset_path / \"labels\"),\n",
    "    ocr_results_dir=str(dataset_path / \"ocr_text\")\n",
    ")\n",
    "\n",
    "print(\"Error Analyzer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Single Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample document IDs\n",
    "labels_dir = dataset_path / \"labels\"\n",
    "label_files = list_files(labels_dir, pattern=\"*.json\", recursive=False)\n",
    "label_files = [f for f in label_files if not f.name.endswith(\"_metadata.json\")]\n",
    "\n",
    "if label_files:\n",
    "    sample_doc_id = label_files[0].stem\n",
    "    print(f\"Analyzing document: {sample_doc_id}\")\n",
    "    \n",
    "    # Analyze errors\n",
    "    errors = analyzer.analyze_field_errors(sample_doc_id, engine=\"paddle\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\nField Analysis:\")\n",
    "        print(\"=\"*80)\n",
    "        for field, error in errors['field_errors'].items():\n",
    "            print(f\"\\n{field.upper()}:\")\n",
    "            print(f\"  Ground Truth: {error['ground_truth']}\")\n",
    "            print(f\"  OCR Extracted: {error['ocr_extracted']}\")\n",
    "            print(f\"  Similarity: {error['similarity']:.2%}\")\n",
    "            print(f\"  Correct: {'✓' if error['is_correct'] else '✗'}\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        print(\"No OCR results found. Run notebook 02 first.\")\n",
    "else:\n",
    "    print(\"No labels found. Generate dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comprehensive Error Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all document IDs\n",
    "doc_ids = [f.stem for f in label_files if not f.name.endswith(\"_metadata.json\")]\n",
    "\n",
    "# Filter to docs with OCR results\n",
    "ocr_dir = dataset_path / \"ocr_text\" / \"paddle\"\n",
    "ocr_files = list_files(ocr_dir, pattern=\"*_ocr.json\", recursive=False)\n",
    "ocr_doc_ids = [f.stem.replace(\"_ocr\", \"\") for f in ocr_files]\n",
    "\n",
    "# Only analyze docs with both labels and OCR results\n",
    "available_doc_ids = [doc_id for doc_id in doc_ids if doc_id in ocr_doc_ids]\n",
    "\n",
    "print(f\"Found {len(available_doc_ids)} documents with OCR results\")\n",
    "\n",
    "if available_doc_ids:\n",
    "    # Generate report\n",
    "    print(\"\\nGenerating error report...\")\n",
    "    report = analyzer.generate_error_report(\n",
    "        engine=\"paddle\",\n",
    "        doc_ids=available_doc_ids\n",
    "    )\n",
    "    \n",
    "    # Save report\n",
    "    report_path = Path.cwd().parent / \"reports\" / \"paddle_error_report.json\"\n",
    "    analyzer.save_report(report, str(report_path))\n",
    "else:\n",
    "    print(\"No OCR results available. Process images in notebook 02 first.\")\n",
    "    report = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Field Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report:\n",
    "    # Extract field accuracies\n",
    "    field_accuracy = report['field_accuracy']\n",
    "    \n",
    "    # Create bar chart\n",
    "    fields = [k for k in field_accuracy.keys() if k != 'overall']\n",
    "    accuracies = [field_accuracy[k] for k in fields]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(fields, accuracies, color=['green' if a >= 0.85 else 'orange' if a >= 0.70 else 'red' for a in accuracies])\n",
    "    plt.axhline(y=0.85, color='r', linestyle='--', label='Target Threshold (85%)')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Field Extraction Accuracy - PaddleOCR')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{acc:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No report available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Error Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report and report['detailed_errors']:\n",
    "    # Collect all field errors\n",
    "    all_errors = []\n",
    "    \n",
    "    for doc_error in report['detailed_errors']:\n",
    "        for field, error in doc_error['field_errors'].items():\n",
    "            all_errors.append({\n",
    "                'doc_id': doc_error['doc_id'],\n",
    "                'field': field,\n",
    "                'similarity': error['similarity'],\n",
    "                'is_correct': error['is_correct']\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_errors = pd.DataFrame(all_errors)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nError Pattern Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nSimilarity Distribution by Field:\")\n",
    "    print(df_errors.groupby('field')['similarity'].describe())\n",
    "    \n",
    "    print(\"\\nError Count by Field:\")\n",
    "    error_counts = df_errors[~df_errors['is_correct']].groupby('field').size()\n",
    "    print(error_counts)\n",
    "    \n",
    "    # Visualize similarity distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    fields = df_errors['field'].unique()\n",
    "    for i, field in enumerate(fields[:4]):\n",
    "        field_data = df_errors[df_errors['field'] == field]\n",
    "        axes[i].hist(field_data['similarity'], bins=20, edgecolor='black')\n",
    "        axes[i].set_title(f'{field} - Similarity Distribution')\n",
    "        axes[i].set_xlabel('Similarity Score')\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].axvline(x=0.85, color='r', linestyle='--', label='Threshold')\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No detailed errors available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Confidence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report and report['detailed_errors']:\n",
    "    # Extract confidence and accuracy relationship\n",
    "    confidence_data = []\n",
    "    \n",
    "    for doc_error in report['detailed_errors']:\n",
    "        avg_confidence = doc_error['avg_confidence']\n",
    "        \n",
    "        for field, error in doc_error['field_errors'].items():\n",
    "            confidence_data.append({\n",
    "                'confidence': avg_confidence,\n",
    "                'similarity': error['similarity'],\n",
    "                'is_correct': error['is_correct']\n",
    "            })\n",
    "    \n",
    "    df_conf = pd.DataFrame(confidence_data)\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['green' if correct else 'red' for correct in df_conf['is_correct']]\n",
    "    plt.scatter(df_conf['confidence'], df_conf['similarity'], c=colors, alpha=0.5)\n",
    "    plt.xlabel('OCR Confidence Score')\n",
    "    plt.ylabel('Field Extraction Similarity')\n",
    "    plt.title('OCR Confidence vs Field Extraction Accuracy')\n",
    "    plt.axhline(y=0.85, color='blue', linestyle='--', label='Similarity Threshold')\n",
    "    plt.legend(['Similarity Threshold', 'Correct', 'Incorrect'])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for confidence analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY FINDINGS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n1. Overall Accuracy: {report['field_accuracy']['overall']:.2%}\")\n",
    "    \n",
    "    print(\"\\n2. Field-Level Performance:\")\n",
    "    for field, accuracy in sorted(report['field_accuracy'].items(), key=lambda x: x[1], reverse=True):\n",
    "        if field != 'overall':\n",
    "            status = \"✓\" if accuracy >= 0.85 else \"⚠\" if accuracy >= 0.70 else \"✗\"\n",
    "            print(f\"   {status} {field}: {accuracy:.2%}\")\n",
    "    \n",
    "    print(f\"\\n3. Average OCR Confidence: {report['summary']['avg_confidence']:.2%}\")\n",
    "    print(f\"4. Average Blocks per Document: {report['summary']['avg_blocks_per_doc']:.1f}\")\n",
    "    \n",
    "    print(\"\\n5. Recommendations:\")\n",
    "    low_accuracy_fields = [f for f, a in report['field_accuracy'].items() \n",
    "                          if f != 'overall' and a < 0.85]\n",
    "    if low_accuracy_fields:\n",
    "        print(f\"   - Focus on improving extraction for: {', '.join(low_accuracy_fields)}\")\n",
    "        print(\"   - Consider field-specific extraction patterns\")\n",
    "        print(\"   - Enhance preprocessing for better OCR quality\")\n",
    "    else:\n",
    "        print(\"   - All fields meet accuracy threshold\")\n",
    "        print(\"   - Consider testing on real-world noisy documents\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"No report available for summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook analyzed OCR errors:\n",
    "- Compared OCR results with ground truth labels\n",
    "- Measured field-level accuracy\n",
    "- Identified error patterns\n",
    "- Correlated confidence scores with accuracy\n",
    "\n",
    "Use these insights for Week 2 improvements!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
